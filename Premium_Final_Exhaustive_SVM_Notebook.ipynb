{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c762c700",
   "metadata": {},
   "source": [
    "# 🧠 Support Vector Machine (SVM) - Exhaustive Polished Notebook\n",
    "\n",
    "Welcome to the **premium exhaustive SVM tutorial notebook**! 🚀\n",
    "We cover deep theory, math, intuition, visualizations, hyperparameters, tuning, metrics, and practical code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af9463",
   "metadata": {},
   "source": [
    "# 📍 1. Learning Roadmap\n",
    "\n",
    "```\n",
    "Real-world Intuition ➔ Why SVM ➔ Core Idea ➔ Math ➔ Margins ➔ Support Vectors ➔ Kernels ➔ Hinge Loss ➔ Code ➔ Error Analysis ➔ Tuning ➔ Roadmap ➔ Mini Projects\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ed09a3",
   "metadata": {},
   "source": [
    "# ✨ 2. Introduction to SVM\n",
    "\n",
    "- SVM = **Separating data with maximum margin**.\n",
    "- Goal: Draw a boundary that separates classes as wide as possible.\n",
    "- Applications: Face recognition, Bioinformatics, Text classification (spam detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c7187",
   "metadata": {},
   "source": [
    "# 🧠 3. Why Do We Need SVM?\n",
    "\n",
    "- Logistic Regression struggles with complex boundaries.\n",
    "- KNN sensitive to noise and scaling.\n",
    "- SVM provides **robust classification**, even in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b68d7",
   "metadata": {},
   "source": [
    "# 🔥 4. Core Idea Behind SVM\n",
    "\n",
    "- Margin: Distance between decision boundary and nearest points.\n",
    "- Maximize margin ➔ Generalization improves.\n",
    "- Support Vectors: Points that define the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c727b3",
   "metadata": {},
   "source": [
    "\n",
    "# 🔢 5. Mathematical Formulation of SVM (Exhaustive)\n",
    "\n",
    "## Objective:\n",
    "$$\n",
    "\\min_{w,b} \\quad \\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "## Subject to:\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i\n",
    "$$\n",
    "\n",
    "## Soft Margin:\n",
    "- Allow slack variables \\( \\xi_i \\).\n",
    "- Updated Objective:\n",
    "$$\n",
    "\\min_{w,b,\\xi} \\quad \\frac{1}{2} \\|w\\|^2 + C \\sum \\xi_i\n",
    "$$\n",
    "\n",
    "## Dual Problem:\n",
    "$$\n",
    "\\max_{\\alpha} \\quad \\sum \\alpha_i - \\frac{1}{2} \\sum \\sum \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j)\n",
    "$$\n",
    "\n",
    "Only support vectors have non-zero \\( \\alpha \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0a0f0",
   "metadata": {},
   "source": [
    "# 📈 Margin Visualization (ASCII)\n",
    "\n",
    "```\n",
    "Class A (o o o)\n",
    "\n",
    "    || Margin ||\n",
    "------------------------------ Hyperplane -------------------------------\n",
    "    || Margin ||\n",
    "\n",
    "Class B (x x x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f057146d",
   "metadata": {},
   "source": [
    "\n",
    "# ⚙️ 6. Hyperparameters of SVM (Fully Exhaustive)\n",
    "\n",
    "## 🔵 1. C (Regularization Parameter)\n",
    "| Aspect | Explanation |\n",
    "|:---|:---|\n",
    "| 📜 Definition | Controls trade-off between margin width and training errors. |\n",
    "| 🎯 Role | Low C ➔ wider margin; High C ➔ tighter margin. |\n",
    "| 📈 Impact | High C ➔ Overfitting; Low C ➔ Underfitting. |\n",
    "| 🔧 Tuning Tip | Try 0.01, 0.1, 1, 10, 100. |\n",
    "| 🔥 Practical Insight | Start lower for noisy data. |\n",
    "\n",
    "## 🔵 2. Kernel\n",
    "| Aspect | Explanation |\n",
    "|:---|:---|\n",
    "| 📜 Definition | Function that maps data into higher-dimensional space. |\n",
    "| 🎯 Role | Defines shape of decision boundary. |\n",
    "| 📈 Impact | Wrong kernel ➔ poor separation. |\n",
    "| 🔧 Tuning Tip | Start with 'rbf', then try 'linear'. |\n",
    "| 🔥 Practical Insight | 90% real-world SVMs use 'rbf'. |\n",
    "\n",
    "## 🔵 3. Gamma\n",
    "| Aspect | Explanation |\n",
    "|:---|:---|\n",
    "| 📜 Definition | Defines influence radius of points. |\n",
    "| 🎯 Role | Controls boundary flexibility. |\n",
    "| 📈 Impact | High gamma ➔ Overfitting; Low gamma ➔ Underfitting. |\n",
    "| 🔧 Tuning Tip | Try 0.001, 0.01, 0.1, 1, 10. |\n",
    "| 🔥 Practical Insight | Always tune C and Gamma together. |\n",
    "\n",
    "## 🔵 4. Degree (for Polynomial Kernel)\n",
    "| Aspect | Explanation |\n",
    "|:---|:---|\n",
    "| 📜 Definition | Degree of polynomial in 'poly' kernel. |\n",
    "| 🎯 Role | Controls complexity of curve. |\n",
    "| 📈 Impact | Higher degree ➔ more complex boundaries. |\n",
    "| 🔧 Tuning Tip | Start with degree=3. |\n",
    "| 🔥 Practical Insight | Prefer RBF over poly unless justified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71965dd3",
   "metadata": {},
   "source": [
    "# 🔁 7. Kernel Trick\n",
    "\n",
    "- Maps data to higher dimensions where it becomes separable.\n",
    "- Common kernels: linear, polynomial, RBF, sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb67e1",
   "metadata": {},
   "source": [
    "# 🔍 8. Hinge Loss Function\n",
    "\n",
    "Loss =\n",
    "$$\n",
    "\\max(0, 1 - y_i (w \\cdot x_i + b))\n",
    "$$\n",
    "- No penalty if correct and outside margin.\n",
    "- Linear penalty if inside margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = np.linspace(-2, 3, 100)\n",
    "hinge_loss = np.maximum(0, 1 - scores)\n",
    "\n",
    "plt.plot(scores, hinge_loss, label=\"Hinge Loss\")\n",
    "plt.axvline(x=1, linestyle=\"--\", color=\"grey\", label=\"Margin\")\n",
    "plt.title(\"Hinge Loss vs Score\")\n",
    "plt.xlabel(\"Score (y*(w.x+b))\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd543d",
   "metadata": {},
   "source": [
    "\n",
    "# 📊 9. Evaluation Metrics (Fully Exhaustive)\n",
    "\n",
    "| Metric | Formula | Best When |\n",
    "|:---|:---|:---|\n",
    "| Accuracy | (TP+TN)/(TP+TN+FP+FN) | Balanced classes |\n",
    "| Precision | TP/(TP+FP) | False positives costly |\n",
    "| Recall | TP/(TP+FN) | False negatives costly |\n",
    "| F1-Score | 2(Precision×Recall)/(Precision+Recall) | Imbalanced data |\n",
    "| ROC-AUC | Prob. ranking | Binary classification |\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
