{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "224e4984",
   "metadata": {},
   "source": [
    "# üß† Support Vector Machine (SVM) - Exhaustive Master Notebook\n",
    "\n",
    "Welcome to the **most exhaustive SVM learning notebook**!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b2e3d",
   "metadata": {},
   "source": [
    "# üìç Learning Roadmap\n",
    "1. What is SVM? (Real-world analogy)\n",
    "2. Mathematical Intuition (Margins, Hyperplanes)\n",
    "3. ASCII Visual Explanation\n",
    "4. Hyperparameters Tuning\n",
    "5. Evaluation Metrics\n",
    "6. Full Hands-On Practice (Easy ‚ûî Medium ‚ûî Complex)\n",
    "7. Cross Validation and Grid Search\n",
    "8. Error Analysis\n",
    "9. Common Mistakes and Solutions\n",
    "10. Where SVM Fits in ML Roadmap\n",
    "11. Final Takeaways\n",
    "12. Exercises for Self-Practice\n",
    "13. Hinge Loss Explanation\n",
    "14. Soft Margin vs Hard Margin\n",
    "15. Kernel Tricks Visualization\n",
    "16. SVM vs Logistic Regression Comparison\n",
    "17. Brief History/Origin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc5c8f",
   "metadata": {},
   "source": [
    "# ‚ú® What is SVM? (Real World Analogy)\n",
    "\n",
    "Imagine a field with red and blue balls üéæüîµ. Your goal is to place a **straight line** that perfectly separates them with the **maximum possible gap** between the nearest points of each class. \n",
    "\n",
    "That 'maximum gap' is the margin, and **SVM is about maximizing this margin** while correctly classifying the points!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1d4cb",
   "metadata": {},
   "source": [
    "# üî¢ Mathematical Intuition Behind SVM\n",
    "- Find a hyperplane that best separates two classes.\n",
    "- Maximize the margin between closest points (support vectors) and the hyperplane.\n",
    "- Optimization Objective:\n",
    "\n",
    "\\[ \\text{Minimize } \\frac{1}{2} ||w||^2 \\quad \\text{subject to } y_i (w \\cdot x_i + b) \\geq 1 \\]\n",
    "\n",
    "where (w) is weight vector and (b) is bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a57de2",
   "metadata": {},
   "source": [
    "# üé® ASCII Visual Explanation\n",
    "```\n",
    "Class A o o o\n",
    "        | Margin |\n",
    "------------Hyperplane--------------\n",
    "        | Margin |\n",
    "Class B x x x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37046c1d",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è SVM Hyperparameters ‚Äî Complete Guide\n",
    "- `C`: Regularization parameter (small C ‚ûî wider margin, more misclassifications allowed)\n",
    "- `kernel`: 'linear', 'poly', 'rbf', 'sigmoid'\n",
    "- `gamma`: Kernel coefficient for 'rbf', 'poly', and 'sigmoid'.\n",
    "- `degree`: Degree for polynomial kernel.\n",
    "- GridSearchCV can be used to tune all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8589300d",
   "metadata": {},
   "source": [
    "# üìä Evaluation Metrics for SVM\n",
    "- Accuracy\n",
    "- Confusion Matrix\n",
    "- Precision, Recall, F1-Score\n",
    "- ROC Curve and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581add39",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Full Hands-On Code (Easy ‚ûî Medium ‚ûî Complex)\n",
    "\n",
    "We'll work with:\n",
    "- üü¢ Easy ‚ûî Iris Dataset\n",
    "- üü° Medium ‚ûî Wine Quality\n",
    "- üî¥ Complex ‚ûî Fashion MNIST\n",
    "\n",
    "We'll also explain each code block step-by-step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: SVM on Iris dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = SVC(kernel='linear', C=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dea930",
   "metadata": {},
   "source": [
    "### üîç Let's understand this code step-by-step:\n",
    "1. Load Iris dataset.\n",
    "2. Split into train/test sets.\n",
    "3. Build SVM with linear kernel.\n",
    "4. Fit on training data.\n",
    "5. Predict and evaluate accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec76cb0f",
   "metadata": {},
   "source": [
    "# üß™ GridSearchCV Tuning for SVM\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [1, 0.1, 0.01],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f06043",
   "metadata": {},
   "source": [
    "# üìâ Error Analysis\n",
    "- Analyze support vectors\n",
    "- Boundary errors (where points are misclassified)\n",
    "- Visualize decision boundaries whenever possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de1b1c7",
   "metadata": {},
   "source": [
    "# üö® Common Mistakes in SVM\n",
    "| Cause | Effect | Solution |\n",
    "|:---|:---|:---|\n",
    "| C too low | High bias | Increase C |\n",
    "| Wrong Kernel | Poor decision boundary | Try different kernels |\n",
    "| Ignoring scaling | Bad performance | Always Standardize features |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df288a0",
   "metadata": {},
   "source": [
    "# üõ§Ô∏è ML Roadmap: Where SVM Fits\n",
    "- Classical ML: Small-medium datasets\n",
    "- Text Classification\n",
    "- Image Recognition\n",
    "- Bioinformatics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc00c91",
   "metadata": {},
   "source": [
    "# üéØ Final Takeaways Checklist\n",
    "‚úÖ Understand margins\n",
    "‚úÖ Practice with different kernels\n",
    "‚úÖ Always scale data\n",
    "‚úÖ Tune C and Gamma carefully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1acbce",
   "metadata": {},
   "source": [
    "# üöÄ Mini Projects/Exercises\n",
    "- Build SVM for classifying hand-written digits.\n",
    "- Use SVM for binary text classification (spam detection).\n",
    "- Visualize margin and support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50ac44",
   "metadata": {},
   "source": [
    "# üî• Hinge Loss Explanation\n",
    "SVM optimization is based on minimizing Hinge Loss:\n",
    "\n",
    "\\[ \\text{Loss} = \\sum \\max(0, 1 - y_i (w \\cdot x_i + b)) \\]\n",
    "\n",
    "Ensures correct classification with a margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9fe2b",
   "metadata": {},
   "source": [
    "# üîÅ Soft Margin vs Hard Margin\n",
    "- Hard Margin: No misclassifications (perfectly clean data).\n",
    "- Soft Margin: Allow some misclassifications to generalize better (realistic)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f5e50",
   "metadata": {},
   "source": [
    "# üèãÔ∏è‚Äç‚ôÇÔ∏è Kernel Tricks Visualization\n",
    "- RBF Kernel: Maps to infinite dimensions.\n",
    "- Polynomial Kernel: Adds higher degree terms.\n",
    "- Idea: Project data into higher-dimensional space where it becomes linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29227c0",
   "metadata": {},
   "source": [
    "# ‚ö° SVM vs Logistic Regression\n",
    "| Feature | SVM | Logistic Regression |\n",
    "|:---|:---|:---|\n",
    "| Loss | Hinge Loss | Log Loss |\n",
    "| Decision Boundary | Maximize Margin | Maximize Likelihood |\n",
    "| Probabilities | No (unless calibrated) | Yes |\n",
    "| Strength | Effective in high-dimensional spaces | Output probabilities easily |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b44c0",
   "metadata": {},
   "source": [
    "# üìú Brief History\n",
    "- Developed by Vapnik and colleagues in the 1990s.\n",
    "- Rooted in Statistical Learning Theory.\n",
    "- Designed for robust, generalized classification."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
