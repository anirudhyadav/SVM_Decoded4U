{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c762c700",
   "metadata": {},
   "source": [
    "# ğŸ§  Support Vector Machine (SVM) - Exhaustive Polished Notebook\n",
    "\n",
    "Welcome to the **premium exhaustive SVM tutorial notebook**! ğŸš€\n",
    "We cover deep theory, math, intuition, visualizations, hyperparameters, tuning, metrics, and practical code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af9463",
   "metadata": {},
   "source": [
    "# ğŸ“ 1. Learning Roadmap\n",
    "\n",
    "```\n",
    "Real-world Intuition â” Why SVM â” Core Idea â” Math â” Margins â” Support Vectors â” Kernels â” Hinge Loss â” Code â” Error Analysis â” Tuning â” Roadmap â” Mini Projects\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ed09a3",
   "metadata": {},
   "source": [
    "# âœ¨ 2. Introduction to SVM\n",
    "\n",
    "- SVM = **Separating data with maximum margin**.\n",
    "- Goal: Draw a boundary that separates classes as wide as possible.\n",
    "- Applications: Face recognition, Bioinformatics, Text classification (spam detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c7187",
   "metadata": {},
   "source": [
    "# ğŸ§  3. Why Do We Need SVM?\n",
    "\n",
    "- Logistic Regression struggles with complex boundaries.\n",
    "- KNN sensitive to noise and scaling.\n",
    "- SVM provides **robust classification**, even in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b68d7",
   "metadata": {},
   "source": [
    "# ğŸ”¥ 4. Core Idea Behind SVM\n",
    "\n",
    "- Margin: Distance between decision boundary and nearest points.\n",
    "- Maximize margin â” Generalization improves.\n",
    "- Support Vectors: Points that define the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c727b3",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸ”¢ 5. Mathematical Formulation of SVM (Exhaustive)\n",
    "\n",
    "## Objective:\n",
    "$$\n",
    "\\min_{w,b} \\quad \\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "## Subject to:\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i\n",
    "$$\n",
    "\n",
    "## Soft Margin:\n",
    "- Allow slack variables \\( \\xi_i \\).\n",
    "- Updated Objective:\n",
    "$$\n",
    "\\min_{w,b,\\xi} \\quad \\frac{1}{2} \\|w\\|^2 + C \\sum \\xi_i\n",
    "$$\n",
    "\n",
    "## Dual Problem:\n",
    "$$\n",
    "\\max_{\\alpha} \\quad \\sum \\alpha_i - \\frac{1}{2} \\sum \\sum \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j)\n",
    "$$\n",
    "\n",
    "Only support vectors have non-zero \\( \\alpha \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0a0f0",
   "metadata": {},
   "source": [
    "# ğŸ“ˆ Margin Visualization (ASCII)\n",
    "\n",
    "```\n",
    "Class A (o o o)\n",
    "\n",
    "    || Margin ||\n",
    "------------------------------ Hyperplane -------------------------------\n",
    "    || Margin ||\n",
    "\n",
    "Class B (x x x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f057146d",
   "metadata": {},
   "source": [
    "\n",
    "# âš™ï¸ 6. Hyperparameters of SVM (Fully Exhaustive)\n",
    "\n",
    "## ğŸ”µ 1. C (Regularization Parameter)\n",
    "| Aspect | Explanation |\n",
    "|:---|:---|\n",
    "| ğŸ“œ Definition | Controls trade-off between margin width and training errors. |\n",
    "| ğŸ¯ Role | Low C â” wider margin; High C â” tighter margin. |\n",
    "| ğŸ“ˆ Impact | High C â” Overfitting; Low C â” Underfitting. |\n",
    "| ğŸ”§ Tuning Tip | Try 0.01, 0.1, 1, 10, 100. |\n",
    "| ğŸ”¥ Practical Insight | Start lower for noisy data. |\n",
    "\n",
    "## ğŸ”µ 2. Kernel\n",
    "| Aspect | Explanation |\n",
    "|:---|:---|\n",
    "| ğŸ“œ Definition | Function that maps data into higher-dimensional space. |\n",
    "| ğŸ¯ Role | Defines shape of decision boundary. |\n",
    "| ğŸ“ˆ Impact | Wrong kernel â” poor separation. |\n",
    "| ğŸ”§ Tuning Tip | Start with 'rbf', then try 'linear'. |\n",
    "| ğŸ”¥ Practical Insight | 90% real-world SVMs use 'rbf'. |\n",
    "\n",
    "## ğŸ”µ 3. Gamma\n",
    "| Aspect | Explanation |\n",
    "|:---|:---|\n",
    "| ğŸ“œ Definition | Defines influence radius of points. |\n",
    "| ğŸ¯ Role | Controls boundary flexibility. |\n",
    "| ğŸ“ˆ Impact | High gamma â” Overfitting; Low gamma â” Underfitting. |\n",
    "| ğŸ”§ Tuning Tip | Try 0.001, 0.01, 0.1, 1, 10. |\n",
    "| ğŸ”¥ Practical Insight | Always tune C and Gamma together. |\n",
    "\n",
    "## ğŸ”µ 4. Degree (for Polynomial Kernel)\n",
    "| Aspect | Explanation |\n",
    "|:---|:---|\n",
    "| ğŸ“œ Definition | Degree of polynomial in 'poly' kernel. |\n",
    "| ğŸ¯ Role | Controls complexity of curve. |\n",
    "| ğŸ“ˆ Impact | Higher degree â” more complex boundaries. |\n",
    "| ğŸ”§ Tuning Tip | Start with degree=3. |\n",
    "| ğŸ”¥ Practical Insight | Prefer RBF over poly unless justified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71965dd3",
   "metadata": {},
   "source": [
    "# ğŸ” 7. Kernel Trick\n",
    "\n",
    "- Maps data to higher dimensions where it becomes separable.\n",
    "- Common kernels: linear, polynomial, RBF, sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb67e1",
   "metadata": {},
   "source": [
    "# ğŸ” 8. Hinge Loss Function\n",
    "\n",
    "Loss =\n",
    "$$\n",
    "\\max(0, 1 - y_i (w \\cdot x_i + b))\n",
    "$$\n",
    "- No penalty if correct and outside margin.\n",
    "- Linear penalty if inside margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = np.linspace(-2, 3, 100)\n",
    "hinge_loss = np.maximum(0, 1 - scores)\n",
    "\n",
    "plt.plot(scores, hinge_loss, label=\"Hinge Loss\")\n",
    "plt.axvline(x=1, linestyle=\"--\", color=\"grey\", label=\"Margin\")\n",
    "plt.title(\"Hinge Loss vs Score\")\n",
    "plt.xlabel(\"Score (y*(w.x+b))\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd543d",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸ“Š 9. Evaluation Metrics (Fully Exhaustive)\n",
    "\n",
    "| Metric | Formula | Best When |\n",
    "|:---|:---|:---|\n",
    "| Accuracy | (TP+TN)/(TP+TN+FP+FN) | Balanced classes |\n",
    "| Precision | TP/(TP+FP) | False positives costly |\n",
    "| Recall | TP/(TP+FN) | False negatives costly |\n",
    "| F1-Score | 2(PrecisionÃ—Recall)/(Precision+Recall) | Imbalanced data |\n",
    "| ROC-AUC | Prob. ranking | Binary classification |\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
